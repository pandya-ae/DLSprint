{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization and Deployment of TensorFlow Models with TensorRT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this workshop you will learn how to use the TensorFlow integration for TensorRT (also known as TF-TRT) to increase inference performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the time you complete this workshop you will be able to:\n",
    "\n",
    "- Optimize several deep learning models with TF-TRT\n",
    "- Describe how TF-TRT optimizes models\n",
    "- Use TF-TRT to optimize models at FP32 precision\n",
    "- Use TF-TRT to optimize models at FP16 precision\n",
    "- Perform calibration for INT8 precision optimization\n",
    "- Perform experiments to understand the impact of conversion parameters on optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to be successful with this workshop, it is assumed you:\n",
    "\n",
    "- Are competent in the Python programming language\n",
    "- Are familiar with Deep Learning, and understand what **inference** is\n",
    "- Are familiar with TensorFlow, and its Keras API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This workshop consists of several JupyterLab Notebooks.\n",
    "\n",
    "- **01-intro.ipynb**: This notebook.\n",
    "- **02-jupyter.ipynb**: (optional) A quick overview of how to work with this JupyterLab environment.\n",
    "- **03-naive-inference.ipynb**: Review inference with TF 2 and get familiar with helper functions used in this workshop.\n",
    "- **04-optimizing-tf-models.ipynb**: Learn how TF-TRT optimizes models for faster inference.\n",
    "- **05-FP32-conversion.ipynb**: Learn the syntax for performing optimization with TF-TRT.\n",
    "- **06-exercise-FP16-conversion.ipynb**: Perform FP16 precision optimization.\n",
    "- **07-INT8-inference.ipynb**: Learn how TF-TRT optimizes with INT8 precision.\n",
    "- **08-exercise-INT8-conversion.ipynb**: Perform data calibration and optimize with INT8 precision.\n",
    "- **09_exercise_min_seg_size_benchmarks.ipynb**: Experiment with the impact of the minimum segment size conversion parameter, and optimize additional models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
